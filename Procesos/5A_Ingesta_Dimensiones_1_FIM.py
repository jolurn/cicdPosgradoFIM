# Databricks notebook source
# MAGIC %md
# MAGIC ### =====================================================
# MAGIC ### CONFIGURACI√ìN INICIAL - INGESTA DE DATOS
# MAGIC ### =====================================================

# COMMAND ----------

dbutils.widgets.removeAll()

# COMMAND ----------

# MAGIC %md
# MAGIC ### Par√°metros y Variables

# COMMAND ----------

dbutils.widgets.text("storageName",".")
dbutils.widgets.text("catalogo",".")

# COMMAND ----------

storageName = dbutils.widgets.get("storageName")
catalogo = dbutils.widgets.get("catalogo")

# COMMAND ----------

ruta_raw = f"abfss://fim-raw@{storageName}.dfs.core.windows.net"

# COMMAND ----------

# MAGIC %md
# MAGIC ### =====================================================
# MAGIC ### INGESTA DIMENSIONES 1 - TABLAS MAESTRAS
# MAGIC ### =====================================================

# COMMAND ----------

# MAGIC %md
# MAGIC ### Definici√≥n de Tablas a Ingresar

# COMMAND ----------

# Tablas dimensiones peque√±as - FASE 1
tablas_dimensiones_1 = [
    "pfimapp_estadocivil",
    "pfimapp_tipodocumento",
    "pfimapp_estadoacademico"
]

# COMMAND ----------

# MAGIC %md
# MAGIC ### Funci√≥n de Ingesta

# COMMAND ----------

from pyspark.sql.functions import current_timestamp

# Funci√≥n para ingestar CSV a Bronze CON mergeSchema
def ingestar_csv_a_bronze(tabla):
 csv_path = f"{ruta_raw}/{tabla}.csv"
 bronze_table = f"{catalogo}.bronze.{tabla}"
 
 print(f"üì• Ingestando: {tabla}")
 
 try:
	 df = (spark.read
		   .option("header", "true")
		   .option("inferSchema", "true")
		   .csv(csv_path)
		   .withColumn("fecha_ingesta", current_timestamp()))
	 
	 df.write.format("delta")\
	   .option("mergeSchema", "true")\
	   .mode("overwrite")\
	   .saveAsTable(bronze_table)
	   
	 count = df.count()
	 print(f"‚úÖ {tabla}: {count} registros")
	 return True
	 
 except Exception as e:
	 print(f"‚ùå Error en {tabla}: {str(e)}")
	 return False

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ejecuci√≥n y Resumen de Ingesta

# COMMAND ----------

# Ejecutar ingesta para tablas dimensiones 1
resultados = []
for tabla in tablas_dimensiones_1:
    resultado = ingestar_csv_a_bronze(tabla)
    resultados.append((tabla, resultado))

# COMMAND ----------

# Resumen de ingesta
print("\nüìä RESUMEN DE INGESTA - DIMENSIONES 1:")
for tabla, estado in resultados:
    icon = "‚úÖ" if estado else "‚ùå"
    print(f"{icon} {tabla}")

# COMMAND ----------

# VERIFICACI√ìN DE DATOS INGERIDOS
print("\nüîç VERIFICACI√ìN DE DATOS INGERIDOS:")

for tabla in tablas_dimensiones_1:
    try:
        # Contar registros en la tabla creada
        count = spark.sql(f"SELECT COUNT(*) as total FROM {catalogo}.bronze.{tabla}").collect()[0]['total']
        
        # Mostrar sample de datos
        print(f"\nüìã {tabla}: {count} registros")
        if count > 0:
            spark.sql(f"SELECT * FROM {catalogo}.bronze.{tabla} LIMIT 3").show()
        else:
            print("‚ùå TABLA VAC√çA - Revisar archivo CSV")
            
    except Exception as e:
        print(f"‚ùå Error verificando {tabla}: {str(e)}")

# VERIFICAR ARCHIVOS EN RAW
print("\nüìÅ VERIFICANDO ARCHIVOS EN RAW:")
try:
    archivos = dbutils.fs.ls(ruta_raw)
    print("‚úÖ Archivos encontrados en RAW:")
    for archivo in archivos:
        print(f"   üìÑ {archivo.name}")
except Exception as e:
    print(f"‚ùå Error accediendo a RAW: {str(e)}")